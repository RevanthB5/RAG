{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ['OPENAI_API_KEY'] = '...'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-representation Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\\n\\nScientific Discovery Agent\\n\\nGenerative Agents Simulation\\n\\nProof-of-Concept Examples\\n\\n\\nChallenges\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\n\\n\\nnlp\\nlanguage-model\\nagent\\nsteerability\\nprompting\\n\\n\\n\\n« \\n\\nAdversarial Attacks on LLMs\\n\\n\\n »\\n\\nPrompt Engineering\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\n\\nThinking about High-Quality Human Data | Lil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil\\'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Thinking about High-Quality Human Data\\n    \\nDate: February 5, 2024  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nHuman Raters ‚Üî Data Quality\\n\\nThe Wisdom of the Crowd\\n\\nRater Agreement\\n\\nRater Disagreement & Two Paradigms\\n\\n\\nData Quality ‚Üî Model Training\\n\\nInfluence Functions\\n\\nPrediction Changes during Training\\n\\nNoisy Cross-Validation\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\n[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper “Vox populi”) and nice feedback. \\uf8ffüôè ]\\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution. The community knows the value of high quality data, but somehow we have this subtle impression that ‚ÄúEveryone wants to do the model work, not the data work‚Äù (Sambasivan et al. 2021).\\n\\nFig. 1. Two directions to approach high data quality.\\nHuman Raters ‚Üî Data Quality#\\nCollecting human data involve a set of operation steps and every step contributes to the data quality:\\n\\nTask design: Design task workflow to improve clarity and reduce complexity. Detailed guidelines are helpful but very long and complicated guidelines demand a decent amount of training to be useful.\\nSelect and train a pool of raters: Select annotators with matched skillset and consistency. Training sessions are necessary. After onboarding, regular feedback and calibration sessions are also needed.\\nCollect and aggregate data. This is the stage where more ML techniques can be applied to clean, filter and smartly aggregate data to identify the true labels.\\n\\n\\nFig. 2. Quality assurance refers to a set of actions that allow one to improve quality by acting on the quality attributes identified in the quality model. (Image source: Daniel et al. 2018)\\nThe Wisdom of the Crowd#\\nVox populi (originally “Vox populi, vox Dei”), a Latin phrase, means the voice of people. A short paper named was the same name was published in 1907 on Nature. It tracked an event at an annual exhibition where a fat ox was selected and people would guess the weight of the ox in order to win a prize if the guess is close to the real number. The middlemost estimate was treated as “the vox populi” and ended up being very close to the true value. The author concluded “This result is, I think, more creditable to the trustworthiness of a democratic judgment than might have been expected.” This is probably the earliest mention of how crowdsourcing (“the wisdom of the crowd”) would work out.\\nAlmost 100 years later, Callison-Burch (2009) did an early study on using Amazon Mechanical Turk (AMT) to run non-expert human evaluation on Machine Translation (MT) tasks and even to rely on non-experts to create new gold reference translations. The setup for human evaluation was simple: Each turker is shown a source sentence, a reference translation, and 5 translations from 5 MT systems. They are asked to rank 5 translations from best to worst. Each task is completed by 5 turkers.\\nUnsurprisingly, there are spammers producing low quality annotation to only optimize the volume. So when measuring the agreement between experts and non-experts, different weighting schemes need to be applied to downweight the contribution of spammers: (1) “weighted by experts”: using agreement rate with experts on a gold set of 10 examples; (2) “weighted by non-experts”: relying on agreement rate with the rest of turkers on the whole dataset.\\nIn a harder task, non-expert human annotators were asked to create new gold reference translations. Callison-Burch designed the task in two stages, where the first stage created new translations with reference to MT outputs and the second one filtered translations that may seem to be gerated by a MT system. The correlation between experts’ and crowdsourced translations is higher than that between expert and MT system outputs.\\n\\nFig. 3. (Left) The agreement rate is measured by comparing each pair of translation sentences (\"A > B\", \"A=B\", \"A < B\") and thus chance agreement is 1/3. The upper bound is set by the expert-expert agreement rate. (Right) Comparison of BLEU score between translations from different sources. LCD (Linguistic Data Consortium) translators provide expert translations. (Image source: Callison-Burch 2009)\\nRater Agreement#\\nWe often think of annotation as targeting a single ground truth and try to evaluate quality against one gold answer with consistent standards. A common practice for finding reliable ground truth labels is to collect multiple labels from multiple raters. Assuming that each rater performs at a different level of quality, we can use a weighted average of annotations but weighted by a proficiency score. This score is often approximated by how often one rater agrees with others.\\nMajority Voting: Taking the majority vote is the simplest way of aggregation, equivalent to taking the mode of a set of labels. In this setting, every annotator is contributing equally.\\nRaw agreement (Tratz & Hovy, 2010): Raw agreement counts the percentage of other people agreeing with them. This is indirectly correlated to majority vote, because all members of the majority class are expected to get higher inter-annotator agreement rate.\\nCohen’s Kappa (Landis & Koch, 1977): Cohen’s kappa measures the inter-rater agreement in the form of $\\\\kappa = (p_o - p_e) / (1 - p_c)$, where $p_o$ is the raw agreement rate and $p_e$ is the agreement by chance. Cohen’s kappa has a correction term for agreeing by chance, but this correction may be overestimated if one label is more prevalent.\\nProbabilistic Graph Modeling: There is a body of work relying on probabilistic graph modeling to model different factors within annotation decisions, e.g. difficulty of the task, task latent topics, rater bias, rater confidence, and then predict the true labels accordingly. Zheng et al. (2017)  compared 17 algorithms on truth inference in crowdsourcing and most of them are probabilistic graph models.\\n\\nMACE (Multi-Annotator Competence Estimation; Hovy et al. 2013) is an early example of using graph modeling to estimate the likelihood of someone acting like a “spammer” by providing random labels. Unsurprisingly in cases when the incentive is misaligned, some annotators may behave as “spammers” to optimize the volume of tasks completed for higher pay. The goal of MACE is to identify spammers. Given a task $i$ and an annotator $j$, $T_i$ is the true label, $A_{ij}$ is the assigned label and $S_{ij}$ models the probability of annotator $j$ spamming. Then the generative process can be represented as belows. The parameter $\\\\theta_j$ defines the trustworthiness of the annotator $j$ (probability of not spamming) and the parameter $\\\\xi_j$  defines how an annotator behaves when they are spamming.\\n\\n\\n$$\\n\\\\begin{align}\\n& \\\\text{for } i = 1 \\\\dots N : \\\\\\\\\\n& \\\\quad T_i \\\\sim \\\\text{Uniform} \\\\\\\\\\n& \\\\quad \\\\text{for } j = 1 \\\\dots M : \\\\\\\\\\n& \\\\quad \\\\quad S_{ij} \\\\sim \\\\text{Bernoulli}(1 - \\\\theta_j) \\\\\\\\\\n& \\\\quad \\\\quad \\\\text{if } S_{ij} = 0 : \\\\\\\\\\n& \\\\quad \\\\quad \\\\quad A_{ij} = T_i \\\\\\\\\\n& \\\\quad \\\\quad \\\\text{else } : \\\\\\\\\\n& \\\\quad \\\\quad \\\\quad A_{ij} \\\\sim \\\\text{Multinomial}(\\\\xi_j) \\\\\\\\\\n\\\\end{align}\\n$$\\n\\nThen we can learn $\\\\theta, \\\\xi$ to maximize the observed data, in the form of the marginal data likelihood, where $A$ is the matrix of annotations, $S$ is the matrix of competence indicators and $T$ is the matrix of true labels:\\n\\n$$\\nP(A; \\\\theta, \\\\xi) = \\\\sum_{T, S} \\\\big[ \\\\prod_{i=1}^N P(T_i) \\\\cdot \\\\prod_{j=1}^M P(S_{ij}; \\\\theta_j) \\\\cdot P(A_{ij} \\\\vert S_{ij}, T_i; \\\\xi_j) \\\\big]\\n$$\\n\\nEither EM (Expectation‚Äìmaximization) or VB (Variational Bayes) can be applied to maximize the above marginal likelihood. During EM optimization, at M-step, a fixed value $\\\\delta$ is added to the fractional counts before normalizing. During VB training, they applied symmetric Beta priors on $\\\\theta_j$  and symmetric Dirichlet priors on $\\\\xi_j$. When recovering the correct answers, we can take majority vote weighted by the annotators’ $\\\\theta$ estimates.\\nRater Disagreement & Two Paradigms#\\nThe aggregation process described above depends on an assumption that there exists one underlying gold answer and thus we can evaluate annotators’ performance accordingly. However, in many topics, especially in safety, social, or cultural areas, people can disagree and often this disagreement is valid and then it comes down to how much we want to apply a strict rule versus embracing diversity.\\nAroyo & Welty (2015) discussed a set of “myths” in the practice of human annotation collection and found all of them somewhat inaccurate, key findings including:\\n\\nOften there is more than one correct interpretation for some samples. We need diverse perspectives via e.g. having multiple people to review annotation quality.\\nDisagreement is not always bad. We should reduce disagreements caused by errors or poorly designed process but other disagreements can give us rich information.\\n\\nIf it is caused by a task not well defined, we should enhance the instruction. However, a more detailed guideline does not resolve innate diversity among opinions.\\n\\n\\nExperts may not always be better than lay people, but they would have a big gap in terms of considering what’s important.\\nGround truth annotations can change in time, especially those related to timely events or news.\\n\\nLater, Rottger et al. (2021) formulated the difference into two contrasting paradigms for data annotation for subjective NLP tasks.\\n\\n\\n\\n\\nDescriptive\\nPrescriptive\\n\\n\\n\\n\\nDefinition\\nEncourage annotator subjectivity, trying to model many beliefs.\\nDiscourage annotator subjectivity, trying to consistently apply one belief.\\n\\n\\nPros\\n- Can help to identify which entries are more subjective;- Embrace diversity\\n- More aligned with standard NLP setup. - Easier to do QC by measuring disagreement or doing label aggregation.\\n\\n\\nCons\\n- Metrics like rater disagreement cannot be used to measure data quality or annotator performance; - Cannot be used for training models that are optimized for outputting one preset behavior.\\n- Expensive and challenging to create high-quality annotation guidelines, which can never be perfect, in practice;- Training annotators to get familiar with guideline in order to apply it properly is also challenging;- Cannot capture an interpretable diversity of beliefs or consistently encode one specific belief.\\n\\n\\n\\nThe descriptive paradigm allows us to understand a number of important effects as well as to account for different perspectives. For example, annotator identity (e.g. African American, LGBTQ) is found to be a statistically significant factor in how they would label identify-related content as toxic (Goyal et al. 2022). Topics can be another main driver for diverse opinions. Wang et al. (2023) studied the human evaluation process of safety of an AI conversation system and compared results between labels by Trust & Safety (T&S) professionals and crowdsourcing annotators. They intentionally collected rich metadata associated with crowd annotators like demographic or behavior information. Comparing T&S expert labels and crowd annotations, they found that agreement rates vary across semantic topics and the level of severity:\\n\\nAgreement rate differs a lot across different topics; ranging from 0.96 on violence/gory to 0.25 on personal topics.\\nAgreement rates are higher on “extreme” and “benign” conversations, given four label options marking “benign”, “debatable”, “moderate” to “extreme”.\\n\\n\\nFig. 4. Correlations between non-expert and expert annotations vary a lot across topics. (Image source: Wang et al. 2023)\\nZhang et al. (2023) proposed a taxonomy of rater disagreement to analyze the root causes. Among the listed causes, disagreement due to stochastic errors or inconsistency on the individual level should be avoided. In cases when a rater gives different labels to the same task when asked multiple times, some of those are most likely caused by human errors. Based on this intuition, the disagreement deconvolution method (Gordon et al. 2021) disentangles stable opinions from errors by anchoring each individual’s opinion to their own primary label and thus encouraging intra-rater consistency.\\n\\nFig. 5. A taxonomy of causes for rater disagreement. (Image source: Zhang et al. 2023)\\nDisagreement deconvolution relies on probabilistic graph modeling:\\n\\nEstimate how often an annotator returns non-primary labels, $p_\\\\text{flip}$\\nPer sample, get an adjusted label distribution $p^*$ of primary labels based on $p_\\\\text{flip}$\\nSample from $p^*$ as a new test set.\\nMeasure performance metrics against the new test set.\\n\\nGiven $C$-category classification, the sampling process of the generative model is stated as follows:\\n\\n$$\\n\\\\begin{aligned}\\ny^*\\\\mid x &\\\\sim \\\\text{Categorial}([C], p^*(y\\\\mid x)) \\\\\\\\\\ny_\\\\text{other}\\\\mid y^* &\\\\sim \\\\text{Categorial}([C]\\\\setminus\\\\{y^*\\\\}, \\\\frac{1}{C-1}) \\\\\\\\\\nz_\\\\text{flip} \\\\mid x &\\\\sim \\\\text{Bernoulli}(p_\\\\text{flip}(x)) \\\\\\\\\\ny\\\\mid y^*, y_\\\\text{other}, z_\\\\text{flip} &= y^* (1 - z_\\\\text{flip}) + y_\\\\text{other} z_\\\\text{flip}\\n\\\\end{aligned}\\n$$\\n\\nGiven the true $p(y\\\\mid x)$ and $p_\\\\text{flip}$ that can be estimated from the data, we would update the label distribution of primary labels:\\n\\n$$\\np^*(y\\\\mid x) = \\\\frac{p(y\\\\mid x) - \\\\frac{p_\\\\text{flip}(x)}{C-1}}{1 - \\\\frac{C \\\\cdot p_\\\\text{flip}(x)}{C - 1}}\\n$$\\n\\nA new test set sampled from $p^*(y \\\\mid x)$ represents the primary labels with individual inconsistency noise removed. It can be used for evaluation, as a noise-free test set.\\nTo capture systematic disagreement among annotators when learning to predict labels, Davani et al. (2021) experimented with a multi-annotator model where predicting each annotator’s labels is treated as one sub-task. Say, the classification task is defined on an annotated dataset $D=(X, A, Y)$, where $X$ is the text instances, $A$ is the set of annotators and $Y$ is the annotation matrix, $y_{ij} \\\\in Y$ represents a binary label assigned by $a_j \\\\in A$ to the sample $x_i \\\\in X$. The majority vote for $x_i$ is denoted as $\\\\bar{y}_{i,}$. The experiment is to train a classification head on top of a pre-trained BERT model and compares 4 setups:\\n\\nBaseline: Directly predict the majority vote $\\\\bar{y}_i$, not using the full annotation matrix $Y$.\\nEnsemble: Train one model per annotator separately to predict $y_{ij}$ and then the results are aggregated by majority vote.\\nMulti-label: Learn to predict $\\\\vert A \\\\vert$ labels to represent all annotators’ labels per sample $\\\\langle y_{i1}, \\\\dots, y_{i\\\\vert A \\\\vert} \\\\rangle$, with a shared MLP layer and then outputs are aggregated.\\nMulti-task: Similar to multi-label, but each annotator’s prediction head is learned from a separated MLP layer, such that we allocate extra compute to learn the difference among annotators.\\n\\nExperiment results on the GHC (Gab Hate Corpus) dataset showed that the multi-task model achieves the best F1 score and also can naturally provide prediction uncertainty estimation, correlated with annotation disagreement.\\n\\nFig. 6. Illustration of different architectures for modeling multiple annotators\\' labels. (Image source: Davani et al. 2021)\\nJury Learning (Gordon et al. 2022) mimics the jury process by modeling the different annotators’ labeling behavior conditioned on their characteristics. Starting with a dataset with labels and demographic characteristics of each labeler, we train a model to learn to predict labels made by every individual annotator, each as a potential juror. At decision time, practitioners can specify the composition of a group of jurors to determine a sampling strategy. The final decision is made by aggregating labels from jurors from multiple trials.\\n\\nFig. 7. Illustration of how jury learning works. (Image source: Gordon et al. 2022)\\nThe jury learning model is a DCN (Deep & Cross network) , commonly for recommendation use case,  that is jointly trained to learn comment embedding, annotator embedding and group (annotator’s characteristics) embedding. The text content is processed by a pre-trained BERT, which is also jointly fine-tuned but for a shorter period to avoid overfitting.\\n\\nFig. 8. DCN model architecture for jury learning. (Image source: Gordon et al. 2022)\\nTheir experiment runs on the toxicity diversity dataset and compares jury learning with a baseline model which is a fine-tuned BERT to predict individual annotator’s label without using metadata. Performance is measured in MAE (mean absolute error). Jury learning consistently outperforms the annotator-agnostic baseline on the full test set as well as each group segment.\\n\\nFig. 9. Experiment results comparing an annotator-agnostic baseline with jury learning. (Image source: Gordon et al. 2022)\\nData Quality ‚Üî Model Training#\\nOnce a dataset is constructed, many methods can help identify mislabels according to the training dynamics. Note that we only focus on methods to find and exclude data points with potentially incorrect labels, not about how to train a model with noisy data.\\nInfluence Functions#\\nInfluence functions is a classic technique from robust statistics (Hampel, 1974) to measure the effect of training data points by describing how the model parameters change as we upweight a training point by an infinitesimal amount. Koh & Liang (2017) introduced the concept to be applied to deep neural networks.\\nGiven $n$ data samples in the train set, $z_i = (x_i, y_i)$ for $i =1, \\\\dots, n$, The model parameter $\\\\theta$ is optimized to minimize a loss: $\\\\hat{\\\\theta} = \\\\arg\\\\min_{\\\\theta \\\\in \\\\Theta} \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\mathcal{L}(z_i, \\\\theta)$. The change of model parameters after we remove a single data point $z$ is denoted as $\\\\hat{\\\\theta}_{-z} - \\\\hat{\\\\theta}$ where $\\\\hat{\\\\theta}_{-z} = \\\\arg\\\\min_{\\\\theta \\\\in \\\\Theta} \\\\frac{1}{n} \\\\sum_{z_i \\\\neq z} \\\\mathcal{L}(z_i, \\\\theta)$. However, computing this literally for every sample is too expensive. One way to approximate this is to compute the parameter change given a small upweight $\\\\epsilon$ on $z$. By definition, the influence of upweighting $z$ by $\\\\epsilon$ is given by:\\n\\n$$\\n\\\\mathcal{I}_{\\\\text{up,params}}(z) = \\\\frac{d\\\\hat{\\\\theta}_{\\\\epsilon,z}}{d\\\\epsilon}\\\\bigg\\\\vert_{\\\\epsilon=0}=-\\\\mathbf{H}^{-1}_{\\\\hat{\\\\theta}} \\\\nabla_\\\\theta \\\\mathcal{L}(z, \\\\hat{\\\\theta})\\n$$\\n\\nwhere $\\\\hat{\\\\theta}_{\\\\epsilon,z} = \\\\arg\\\\min_{\\\\theta \\\\in \\\\Theta} \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\mathcal{L}(z_i, \\\\theta) + \\\\epsilon L(z, \\\\theta)$ and $\\\\mathbf{H}^{-1}_{\\\\hat{\\\\theta}} = \\\\frac{1}{n}\\\\sum_{i=1}^n \\\\nabla^2_\\\\theta \\\\mathcal{L}(z_i, \\\\hat{\\\\theta})$.\\nRemoving a data point $x$ is equivalent to upweighting it by $\\\\epsilon = -\\\\frac{1}{n}$ and therefore $\\\\hat{\\\\theta}_{-z} - \\\\hat{\\\\theta} \\\\approx -\\\\frac{1}{n} \\\\mathcal{I}_{\\\\text{up,params}}(z)$.\\nThe influence of upweighting $z$ on the loss at a test point $z_\\\\text{test}$ is given by applying the chain rule:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathcal{I}_{\\\\text{up,loss}}(z, z_\\\\text{test}) \\n&= \\\\frac{d \\\\mathcal{L}(z_\\\\text{test}, \\\\hat{\\\\theta}_{\\\\epsilon,z})}{d\\\\epsilon}\\\\bigg\\\\vert_{\\\\epsilon=0} \\\\\\\\\\n&= \\\\nabla_\\\\theta \\\\mathcal{L}(z_\\\\text{test}, \\\\hat{\\\\theta})^\\\\top \\\\frac{d \\\\hat{\\\\theta}_{\\\\epsilon,z}}{d\\\\epsilon}\\\\bigg\\\\vert_{\\\\epsilon=0} \\\\\\\\\\n&= - \\\\nabla_\\\\theta \\\\mathcal{L}(z_\\\\text{test}, \\\\hat{\\\\theta})^\\\\top \\\\mathbf{H}^{-1}_{\\\\hat{\\\\theta}} \\\\nabla_\\\\theta \\\\mathcal{L}(z, \\\\hat{\\\\theta})\\n\\\\end{aligned}\\n$$\\n\\nUsing the influence function we can measure the effect of a single data point on model parameters and loss function in closed forms. It can help approximate leave-one-out retraining without actually running all the retraining. To identify mislabeled data, we can measure $\\\\mathcal{I}_\\\\text{up,loss}(z_i, z_i)$, approximating the prediction error on $z_i$ if $z_i$ is removed from the training set.\\n\\nFig. 10. Influence functions values match leave-one-out training results on 10-class MNIST. (Image source: Kohn & Liang, 2017)\\nGiven the closed form, influence functions is still hard to be scaled up because the inverse Hessian vector product is hard to compute. Grosse et al. (2023) experimented with the EK-FAC (Eigenvalue-corrected Kronecker-Factored Approximate Curvature; George et al. 2018) approximation instead.\\nPrediction Changes during Training#\\nAnother branch of methods are to track the changes of model prediction during training to identify cases which seem hard to be learned. Data Maps (Swayamdipta et al. 2020) tracks two attributes of model behavior dynamics during training to analyze the quality of dataset:\\n\\nConfidence: The model’s confidence in the true label, defined as the mean model probability of the true label across epochs. They also used a coarse-grained metric, “correctness”, defined as the fraction of times when the model predicts the correct label across epochs.\\nVariability: The variation of the confidence, defined as the standard deviation of model probability of the true label across epochs.\\n\\n\\nFig. 11. Data map for SNLI training set, based on a RoBERTa classifier. (Image source: Swayamdipta et al. 2020)\\nHard-to-learn (low confidence, low variability) samples are more likely to be mislabeled. They ran an experiment on WinoGrande dataset with 1% flipped label data. After retraining, flipped instances move to the lower confidence and slightly higher variability regions, indicating that the hard-to-learn regions contains mislabeled samples. Given this, we can train a classifier on equal numbers of label flipped and clean samples using only the confidence score (unsure why the paper didn’t use both confidence and variability as features). This simple noise classifier then can be used on the original dataset to identify potentially mislabeled instances.\\n\\nFig. 12. Data points originally with high confidence and low variability scores moved to low confidence, slightly higher variability regions after labels get flipped. (Image source: Swayamdipta et al. 2020)\\nHowever, we should not consider all hard-to-learn samples to be incorrect. In fact, the paper hypothesizes that ambiguous (high variability) and hard-to-learn (low confidence, low variability) samples are more informative for learning. Experiments showed that they are good for OOD generalization, giving better results on OOD eval, even in comparison to 100% training set.\\nTo investigate whether neural networks have a tendency to forget previously learned information, Mariya Toneva et al. (2019) designed an experiment: They track the model prediction for each sample during the training process and count the transitions for each sample from being classified correctly to incorrectly or vice-versa. Then samples can be categorized accordingly,\\n\\nForgettable (redundant) samples: If the class label changes across training epochs.\\nUnforgettable samples: If the class label assignment is consistent across training epochs. Those samples are never forgotten once learned.\\n\\nThey found that there are a large number of unforgettable examples that are never forgotten once learnt. Examples with noisy labels or images with “uncommon” features (visually complicated to classify) are among the most forgotten examples. The experiments empirically validated that unforgettable examples can be safely removed without compromising model performance.\\nIn the implementation, the forgetting event is only counted when a sample is included in the current training batch; that is, they compute forgetting across presentations of the same example in subsequent mini-batches. The number of forgetting events per sample is quite stable across different seeds and forgettable examples have a small tendency to be first-time learned later in the training. The forgetting events are also found to be transferable throughout the training period and between architectures.\\nPleiss, et al. (2020) developed a method named AUM (Area under the Margin) to spot wrong labels based on such an assumption: Say, a BIRD image is mistakenly marked as DOG. The gradient update would encourage generalization from other BIRD images to this BIRD image, while the DOG label provides an incorrect supervised signal to encourage the update to go another way. Hence, there exists tension between generalization and (wrong) prediction in gradient update signals.\\nGiven a classification dataset $(\\\\mathbf{x}, y) \\\\in \\\\mathcal{D}_\\\\text{train}$, let $z^{(t)}_i(\\\\mathbf{x}) \\\\in \\\\mathbb{R}$ be the logit corresponding to class $i$ at epoch $t$. The margin at epoch $t$ is the difference between the assigned logit and the next largest logit:\\n\\n$$\\nM^{(t)}(\\\\mathbf{x}, y) = z_y^{(t)}(\\\\mathbf{x}) - \\\\max_{i \\\\neq y} z^{(t)}_i(\\\\mathbf{x}),\\\\quad\\n\\\\text{AUM}(\\\\mathbf{x}, y) = \\\\frac{1}{T} \\\\sum^T_{t=1} M^{(t)}(\\\\mathbf{x}, y)\\n$$\\n\\nA negative margin indicates a wrong prediction and a large positive margin suggests high confidence in a correct prediction. The hypothesis is that mislabeled samples would have a smaller margin than correct samples due to the tension of generalization via SGD triggered by other samples.\\nIn order to determine the threshold, they insert fake data, named “threshold samples”, to determine the threshold:\\n\\nCreate a subset of threshold samples $\\\\mathcal{D}_\\\\text{thr}$.  If there are $N$ training samples for $C$ classes, we randomly sample $N/(C+1)$ samples and switch all their labels to a fake new class $C+1$.\\nMerge threshold samples into the original dataset: $\\\\mathcal{D}’ = { (\\\\mathbf{x}, C+1): \\\\mathbf{x} \\\\in \\\\mathcal{D}_\\\\text{thr}} \\\\cup (\\\\mathcal{D} \\\\setminus\\\\mathcal{D}_\\\\text{thr})$;\\nTrain the model on $\\\\mathcal{D}’$ and measure AUM of all the data;\\nCompute the threshold $\\\\alpha$ as the 99th percentile of AUM of threshold samples;\\nIdentify mislabeled data using $\\\\alpha$ a threshold: ${(\\\\mathbf{x}, y) \\\\in \\\\mathcal{D} \\\\setminus \\\\mathcal{D}_\\\\text{thr}: \\\\text{AUM}_{\\\\mathbf{x}, y} \\\\leq \\\\alpha}$\\n\\n\\nFig. 13. How the AUM of threshold samples help separate out mislabeled samples. (Image source: Pleiss et al. 2020)\\n\\nFig. 14. Test error on CIFAR 10/100 with randomly mislabeled samples, comparing different methods for data filter or noisy data training. (Image source: Pleiss et al. 2020)\\nNoisy Cross-Validation#\\nThe NCV (Noisy Cross-Validation) method (Chen et al. 2019) divides the dataset into half at random, and then identifies data samples as “clean” if its label matches the predicted label provided by the model that is only trained on the other half of the dataset. Clean samples are expected to be more trustworthy. INCV (Iterative Noisy Cross-Validation) runs NCV iteratively where more clean samples are added into the trusted candidate set $\\\\mathcal{C}$ and more noisy samples are removed.\\n\\nFig. 15. Algorithm of INCV (iterative noisy cross-validation). (Image source: Chen et al. 2019)\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Feb 2024). “Thinking about High-Quality Human Data”. Lil’Log. https://lilianweng.github.io/posts/2024-02-05-human-data-quality/.\\n\\nOr\\n@article{weng2024humandata,\\n  title   = \"Thinking about High-Quality Human Data\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Feb\",\\n  url     = \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\"\\n}\\nReferences#\\n[1] Francis Galton “Vox populi”  Nature 75, 450-451 (1907).\\n[2] Sambasivan et al. “Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI\" CHI 2021\\n[3] Chris Callison-Burch. “Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon‚Äôs Mechanical Turk” EMNLP 2009\\n[4] Rottger et al. “Two Contrasting Data Annotation Paradigms for Subjective NLP Tasks” NAACL 2022.\\n[5] Aroyo & Welty “Truth Is a Lie: Crowd Truth and the Seven Myths of Human Annotation” AI Magazine¬†36.1: 15-24 (2015).\\n[6] Hovy et al. “Learning Whom to Trust with MACE” NAACL-HLT 2013.\\n[7] Wang et al. “All that Agrees Is Not Gold: Evaluating Ground Truth Labels and Dialogue Content for Safety” 2023.\\n[8] Zhang et al. “A Taxonomy of Rater Disagreements: Surveying Challenges & Opportunities from the Perspective of Annotating Online Toxicity” arXiv preprint arXiv:2311.04345¬†(2023).\\n[9] Davani et al. “Dealing with disagreements: Looking beyond the majority vote in subjective annotations” ACL 2022.\\n[10] Gordon et al. “Jury Learning: Integrating Dissenting Voices into Machine Learning Models” CHI 2022.\\n[11] Gordon et al. “The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality” CHI 2021\\n[12] Daniel et al. 2018 “Quality Control in Crowdsourcing: A Survey of Quality Attributes, Assessment Techniques, and Assurance Actions” ACM Computing Surveys (CSUR), 51(1), 1-40 (2018).\\n[13] Koh & Liang. “Understanding Black-box Predictions via Influence Functions” ICML 2017.\\n[14] Grosse et al. “Studying Large Language Model Generalization with Influence Functions” arXiv preprint arXiv:2308.03296 (2023).\\n[15] Swayamdipta et al. “Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics” EMNLP 2020.\\n[16] Toneva, et al. “An Empirical Study of Example Forgetting during Deep Neural Network Learning” ICLR 2019.\\n[17] Pleiss, et al.  “Identifying Mislabeled Data using the Area Under the Margin Ranking” NeuriPS 2020.\\n[18] Chen et al. “Understanding and utilizing deep neural networks trained with noisy labels” ICML 2019.\\n\\n\\n\\ndata\\ndata-quality\\nhuman-ai\\n\\n\\n\\n¬´ \\n\\nDiffusion Models for Video Generation\\n\\n\\n ¬ª\\n\\nAdversarial Attacks on LLMs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2024 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'title': \"Thinking about High-Quality Human Data | Lil'Log\", 'description': '[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper “Vox populi”) and nice feedback. \\uf8ffüôè ]\\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution.', 'language': 'en'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc': <function __main__.<lambda>(x)>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"doc\": lambda x: x.page_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\",max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})\n",
    "#The batch method is used to process a list of documents in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The document discusses the concept of building autonomous agents powered by LLM (large language model) and explores key components such as planning, memory, and tool use. It includes case studies, proof-of-concept examples, and challenges faced in implementing LLM-powered agents. The document also provides references to related research papers and resources.',\n",
       " 'The document discusses the importance of high-quality human data for training deep learning models. It covers topics such as collecting human data, the wisdom of the crowd, rater agreement, and disagreement, as well as two paradigms for data annotation. Various methods for ensuring data quality and identifying mislabeled data during model training are also discussed, including influence functions, prediction changes during training, and noisy cross-validation. The document provides references and citations for further reading.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# giving unique IDs to each doc\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=summary, metadata={id_key: doc_ids[idx]}) for idx, summary in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='The document discusses the concept of building autonomous agents powered by LLM (large language model) and explores key components such as planning, memory, and tool use. It includes case studies, proof-of-concept examples, and challenges faced in implementing LLM-powered agents. The document also provides references to related research papers and resources.', metadata={'doc_id': '4717eee7-8fb8-432d-85da-afdd7b2249b7'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Memory in agents'\n",
    "\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RAPTOR paper presents an approach for indexing and retrieval of documents:\n",
    "\n",
    "The leafs are a set of starting documents\n",
    "Leafs are embedded and clustered\n",
    "Clusters are then summarized into higher level (more abstract) consolidations of information across similar documents\n",
    "This process is done recursivly, resulting in a \"tree\" going from raw docs (leafs) to more abstract summaries.\n",
    "\n",
    "We can apply this at varying scales; leafs can be:\n",
    "\n",
    "- Text chunks from a single doc\n",
    "- Full docs\n",
    "With longer context LLMs, it's possible to perform this over full documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "# LCEL docs\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# LCEL w/ PydanticOutputParser (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_pydantic = loader.load()\n",
    "\n",
    "# LCEL w/ Self Query (outside the primary LCEL docs)\n",
    "url = \"https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=1, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs_sq = loader.load()\n",
    "\n",
    "# Doc texts\n",
    "docs.extend([*docs_pydantic, *docs_sq])\n",
    "docs_texts = [d.page_content for d in docs]\n",
    "\n",
    "# Calculate the number of tokens for each document\n",
    "counts = [num_tokens_from_string(d, \"cl100k_base\") for d in docs_texts]\n",
    "\n",
    "# Plotting the histogram of token counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(counts, bins=30, color=\"blue\", edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.xlabel(\"Token Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis=\"y\", alpha=0.75)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest â€œprompt + LLMâ€� chain to the most complex chains (weâ€™ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. Weâ€™re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains itâ€™s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and itâ€™s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'content_type': 'text/html; charset=utf-8', 'title': 'LangChain Expression Language (LCEL) | ğŸ¦œï¸�ğŸ”— LangChain', 'description': 'LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nTutorials | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain 0.2 is out! Leave feedback on the v0.2 docs here. You can view the v0.1 docs here.IntegrationsAPI ReferenceMorePeopleContributingTemplatesCookbooks3rd party tutorialsYouTubearXivv0.2v0.2v0.1\\uf8ffü¶úÔ∏è\\uf8ffüîóLangSmithLangSmith DocsTemplates GitHubTemplates HubLangChain HubJS/TS Docs\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create custom toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun LLMs locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to migrate chains to LCELHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to use a model to call toolstool_calling_parallelHow to force tool calling behaviorHow to pass tool outputs to the modelHow to pass run time values to a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsOverviewRelease PolicyPackagesv0.2LangChain v0.2astream_events v2ChangesSecurityTutorialsOn this pageTutorialsNew to LangChain or to LLM app development in general? Read this material to quickly get up and running.Basics‚ÄãBuild a Simple LLM Application with LCELBuild a ChatbotBuild vector stores and retrieversBuild an AgentWorking with external knowledge‚ÄãBuild a Retrieval Augmented Generation (RAG) ApplicationBuild a Conversational RAG ApplicationBuild a Question/Answering system over SQL dataBuild a Query Analysis SystemBuild a local RAG applicationBuild a Question Answering application over a Graph DatabaseBuild a PDF ingestion and Question/Answering systemSpecialized tasks‚ÄãBuild an Extraction ChainGenerate synthetic dataClassify text into labelsSummarize textLangGraph‚ÄãLangGraph is an extension of LangChain aimed at\\nbuilding robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.LangGraph documentation is currently hosted on a separate site.\\nYou can peruse LangGraph tutorials here.LangSmith‚ÄãLangSmith allows you to closely trace, monitor and evaluate your LLM application.\\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.LangSmith documentation is hosted on a separate site.\\nYou can peruse LangSmith tutorials here.Evaluation‚ÄãLangSmith helps you evaluate the performance of your LLM applications. The below tutorial is a great way to get started:Evaluate your LLM applicationMore‚ÄãFor more tutorials, see our cookbook section.Edit this pageWas this page helpful?You can also leave detailed feedback on GitHub.PreviousIntroductionNextBuild a Question Answering application over a Graph DatabaseBasicsWorking with external knowledgeSpecialized tasksLangGraphLangSmithEvaluationMoreCommunityDiscordTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/v0.2/docs/tutorials/', 'content_type': 'text/html; charset=utf-8', 'title': 'Tutorials | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'New to LangChain or to LLM app development in general? Read this material to quickly get up and running.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nSelf-querying | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1\\uf8ffü¶úÔ∏è\\uf8ffüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs\\uf8ffüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started‚ÄãFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our self-querying retriever‚ÄãNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out‚ÄãAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k‚ÄãWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Constructing from scratch with LCEL‚ÄãTo see what\\'s going on under the hood, and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParserget_query_constructor_promptLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:\\ndummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/', 'content_type': 'text/html; charset=utf-8', 'title': 'Self-querying | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Head to Integrations for documentation on vector stores with built-in support for self-querying.', 'language': 'en'})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens in all context: 5668\n"
     ]
    }
   ],
   "source": [
    "# Doc texts concat\n",
    "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
    "d_reversed = list(reversed(d_sorted))\n",
    "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
    "    [doc.page_content for doc in d_reversed]\n",
    ")\n",
    "print(\n",
    "    \"Num tokens in all context: %s\"\n",
    "    % num_tokens_from_string(concatenated_content, \"cl100k_base\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc texts split\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size_tok = 1000\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=chunk_size_tok, chunk_overlap=0\n",
    ")\n",
    "texts_split = text_splitter.split_text(concatenated_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quickstart | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain',\n",
       " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1\\uf8ffü¶úÔ∏è\\uf8ffüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs\\uf8ffüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersQuickstartOutput ParsersCustom Output ParserstypesRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversIndexingCompositionToolsAgentsChainsMoreComponentsModel I/OOutput parsersQuickstartOn this pageQuickstartLanguage models output text. But many times you may want to get more structured information than just text back. This is where output parsers come in.Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\"Get format instructions\": A method which returns a string containing instructions for how the output of a language model should be formatted.\"Parse\": A method which takes in a string (assumed to be the response from a language model) and parses it into some structure.And then one optional one:\"Parse with prompt\": A method which takes in a string (assumed to be the response from a language model) and a prompt (assumed to be the prompt that generated such a response) and parses it into some structure. The prompt is largely provided in the event the OutputParser wants to retry or fix the output in some way, and needs information from the prompt to do so.Get started‚ÄãBelow we go over the main type of output parser, the PydanticOutputParser.from langchain.output_parsers import PydanticOutputParserfrom langchain_core.prompts import PromptTemplatefrom langchain_core.pydantic_v1 import BaseModel, Field, validatorfrom langchain_openai import OpenAImodel = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)# Define your desired data structure.class Joke(BaseModel):    setup: str = Field(description=\"question to set up a joke\")    punchline: str = Field(description=\"answer to resolve the joke\")    # You can add custom validation logic easily with Pydantic.    @validator(\"setup\")    def question_ends_with_question_mark(cls, field):        if field[-1] != \"?\":            raise ValueError(\"Badly formed question!\")        return field# Set up a parser + inject instructions into the prompt template.parser = PydanticOutputParser(pydantic_object=Joke)prompt = PromptTemplate(    template=\"Answer the user query.\\\\n{format_instructions}\\\\n{query}\\\\n\",    input_variables=[\"query\"],    partial_variables={\"format_instructions\": parser.get_format_instructions()},)# And a query intended to prompt a language model to populate the data structure.prompt_and_model = prompt | modeloutput = prompt_and_model.invoke({\"query\": \"Tell me a joke.\"})parser.invoke(output)API Reference:PydanticOutputParserPromptTemplateOpenAIJoke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')LCEL‚ÄãOutput parsers implement the Runnable interface, the basic building block of the LangChain Expression Language (LCEL). This means they support invoke, ainvoke, stream, astream, batch, abatch, astream_log calls.Output parsers accept a string or BaseMessage as input and can return an arbitrary type.parser.invoke(output)Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')Instead of manually invoking the parser, we also could\\'ve just added it to our Runnable sequence:chain = prompt | model | parserchain.invoke({\"query\": \"Tell me a joke.\"})Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')While all',\n",
       " 'parsers support the streaming interface, only certain parsers can stream through partially parsed objects, since this is highly dependent on the output type. Parsers which cannot construct partial objects will simply yield the fully parsed output.The SimpleJsonOutputParser for example can stream through partial outputs:from langchain.output_parsers.json import SimpleJsonOutputParserjson_prompt = PromptTemplate.from_template(    \"Return a JSON object with an `answer` key that answers the following question: {question}\")json_parser = SimpleJsonOutputParser()json_chain = json_prompt | model | json_parserAPI Reference:SimpleJsonOutputParserlist(json_chain.stream({\"question\": \"Who invented the microscope?\"}))[{}, {\\'answer\\': \\'\\'}, {\\'answer\\': \\'Ant\\'}, {\\'answer\\': \\'Anton\\'}, {\\'answer\\': \\'Antonie\\'}, {\\'answer\\': \\'Antonie van\\'}, {\\'answer\\': \\'Antonie van Lee\\'}, {\\'answer\\': \\'Antonie van Leeu\\'}, {\\'answer\\': \\'Antonie van Leeuwen\\'}, {\\'answer\\': \\'Antonie van Leeuwenho\\'}, {\\'answer\\': \\'Antonie van Leeuwenhoek\\'}]While the PydanticOutputParser cannot:list(chain.stream({\"query\": \"Tell me a joke.\"}))[Joke(setup=\\'Why did the chicken cross the road?\\', punchline=\\'To get to the other side!\\')]Help us out by providing feedback on this documentation page:PreviousOutput ParsersNextOutput ParsersGet startedLCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.',\n",
       " '--- \\n\\n\\n\\n\\n\\n\\n\\nSelf-querying | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain',\n",
       " 'Skip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1\\uf8ffü¶úÔ∏è\\uf8ffüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docs\\uf8ffüí¨SearchModel I/OPromptsChat modelsLLMsOutput parsersRetrievalDocument loadersText splittersEmbedding modelsVector storesRetrieversVector store-backed retrieverRetrieversMultiQueryRetrieverContextual compressionCustom RetrieverEnsemble RetrieverLong-Context ReorderMultiVector RetrieverParent Document RetrieverSelf-queryingTime-weighted vector store retrieverIndexingCompositionToolsAgentsChainsMoreComponentsRetrievalRetrieversSelf-queryingOn this pageSelf-queryinginfoHead to Integrations for documentation on vector stores with built-in support for self-querying.A self-querying retriever is one that, as the name suggests, has the ability to query itself. Specifically, given any natural language query, the retriever uses a query-constructing LLM chain to write a structured query and then applies that structured query to its underlying VectorStore. This allows the retriever to not only use the user-input query for semantic similarity comparison with the contents of stored documents but to also extract filters from the user query on the metadata of stored documents and to execute those filters.Get started‚ÄãFor demonstration purposes we\\'ll use a Chroma vector store. We\\'ve created a small demo set of documents that contain summaries of movies.Note: The self-query retriever requires you to have lark package installed.%pip install --upgrade --quiet  lark langchain-chromafrom langchain_chroma import Chromafrom langchain_core.documents import Documentfrom langchain_openai import OpenAIEmbeddingsdocs = [    Document(        page_content=\"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",        metadata={\"year\": 1993, \"rating\": 7.7, \"genre\": \"science fiction\"},    ),    Document(        page_content=\"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",        metadata={\"year\": 2010, \"director\": \"Christopher Nolan\", \"rating\": 8.2},    ),    Document(        page_content=\"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",        metadata={\"year\": 2006, \"director\": \"Satoshi Kon\", \"rating\": 8.6},    ),    Document(        page_content=\"A bunch of normal-sized women are supremely wholesome and some men pine after them\",        metadata={\"year\": 2019, \"director\": \"Greta Gerwig\", \"rating\": 8.3},    ),    Document(        page_content=\"Toys come alive and have a blast doing so\",        metadata={\"year\": 1995, \"genre\": \"animated\"},    ),    Document(        page_content=\"Three men walk into the Zone, three men walk out of the Zone\",        metadata={            \"year\": 1979,            \"director\": \"Andrei Tarkovsky\",            \"genre\": \"thriller\",            \"rating\": 9.9,        },    ),]vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())API Reference:DocumentOpenAIEmbeddingsCreating our self-querying retriever‚ÄãNow we can instantiate our retriever. To do this we\\'ll need to provide some information upfront about the metadata fields that our documents support and a short description of the document contents.from',\n",
       " 'langchain.chains.query_constructor.base import AttributeInfofrom langchain.retrievers.self_query.base import SelfQueryRetrieverfrom langchain_openai import ChatOpenAImetadata_field_info = [    AttributeInfo(        name=\"genre\",        description=\"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        type=\"string\",    ),    AttributeInfo(        name=\"year\",        description=\"The year the movie was released\",        type=\"integer\",    ),    AttributeInfo(        name=\"director\",        description=\"The name of the movie director\",        type=\"string\",    ),    AttributeInfo(        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"    ),]document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)API Reference:AttributeInfoSelfQueryRetrieverChatOpenAITesting it out‚ÄãAnd now we can actually try using our retriever!# This example only specifies a filterretriever.invoke(\"I want to watch a movie rated higher than 8.5\")[Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979}), Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006})]# This example specifies a query and a filterretriever.invoke(\"Has Greta Gerwig directed any movies about women\")[Document(page_content=\\'A bunch of normal-sized women are supremely wholesome and some men pine after them\\', metadata={\\'director\\': \\'Greta Gerwig\\', \\'rating\\': 8.3, \\'year\\': 2019})]# This example specifies a composite filterretriever.invoke(\"What\\'s a highly rated (above 8.5) science fiction film?\")[Document(page_content=\\'A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\\', metadata={\\'director\\': \\'Satoshi Kon\\', \\'rating\\': 8.6, \\'year\\': 2006}), Document(page_content=\\'Three men walk into the Zone, three men walk out of the Zone\\', metadata={\\'director\\': \\'Andrei Tarkovsky\\', \\'genre\\': \\'thriller\\', \\'rating\\': 9.9, \\'year\\': 1979})]# This example specifies a query and composite filterretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Filter k‚ÄãWe can also use the self query retriever to specify k: the number of documents to fetch.We can do this by passing enable_limit=True to the constructor.retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,    enable_limit=True,)# This example only specifies a relevant queryretriever.invoke(\"What are two movies about dinosaurs\")[Document(page_content=\\'A bunch of scientists bring back dinosaurs and mayhem breaks loose\\', metadata={\\'genre\\': \\'science fiction\\', \\'rating\\': 7.7, \\'year\\': 1993}), Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Constructing from scratch with LCEL‚ÄãTo see what\\'s going on under the hood,',\n",
       " 'and to have more custom control, we can reconstruct our retriever from scratch.First, we need to create a query-construction chain. This chain will take a user query and generated a StructuredQuery object which captures the filters specified by the user. We provide some helper functions for creating a prompt and output parser. These have a number of tunable params that we\\'ll ignore here for simplicity.from langchain.chains.query_constructor.base import (    StructuredQueryOutputParser,    get_query_constructor_prompt,)prompt = get_query_constructor_prompt(    document_content_description,    metadata_field_info,)output_parser = StructuredQueryOutputParser.from_components()query_constructor = prompt | llm | output_parserAPI Reference:StructuredQueryOutputParserget_query_constructor_promptLet\\'s look at our prompt:print(prompt.format(query=\"dummy question\"))Your goal is to structure the user\\'s query to match the request schema provided below.<< Structured Request Schema >>When responding use a markdown code snippet with a JSON object formatted in the following schema:```json{    \"query\": string \\\\ text string to compare to document contents    \"filter\": string \\\\ logical condition statement for filtering documents}The query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.A logical condition statement is composed of one or more comparison and logical operation statements.A comparison statement takes the form: comp(attr, val):comp (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparatorattr (string):  name of attribute to apply the comparison toval (string): is the comparison valueA logical operation statement takes the form op(statement1, statement2, ...):op (and | or | not): logical operatorstatement1, statement2, ... (comparison statements or logical operation statements): one or more statements to apply the operation toMake sure that you only use the comparators and logical operators listed above and no others.',\n",
       " 'Make sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format YYYY-MM-DD when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \"NO_FILTER\" for the filter value.<< Example 1. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genreStructured Request:{    \"query\": \"teenager love\",    \"filter\": \"and(or(eq(\\\\\"artist\\\\\", \\\\\"Taylor Swift\\\\\"), eq(\\\\\"artist\\\\\", \\\\\"Katy Perry\\\\\")), lt(\\\\\"length\\\\\", 180), eq(\\\\\"genre\\\\\", \\\\\"pop\\\\\"))\"}<< Example 2. >>\\nData Source:{    \"content\": \"Lyrics of a song\",    \"attributes\": {        \"artist\": {            \"type\": \"string\",            \"description\": \"Name of the song artist\"        },        \"length\": {            \"type\": \"integer\",            \"description\": \"Length of the song in seconds\"        },        \"genre\": {            \"type\": \"string\",            \"description\": \"The song genre, one of \"pop\", \"rock\" or \"rap\"\"        }    }}User Query:\\nWhat are songs that were not published on SpotifyStructured Request:{    \"query\": \"\",    \"filter\": \"NO_FILTER\"}<< Example 3. >>\\nData Source:{    \"content\": \"Brief summary of a movie\",    \"attributes\": {    \"genre\": {        \"description\": \"The genre of the movie. One of [\\'science fiction\\', \\'comedy\\', \\'drama\\', \\'thriller\\', \\'romance\\', \\'action\\', \\'animated\\']\",        \"type\": \"string\"    },    \"year\": {        \"description\": \"The year the movie was released\",        \"type\": \"integer\"    },    \"director\": {        \"description\": \"The name of the movie director\",        \"type\": \"string\"    },    \"rating\": {        \"description\": \"A 1-10 rating for the movie\",        \"type\": \"float\"    }}}User Query:',\n",
       " 'dummy questionStructured Request:And what our full chain produces:```pythonquery_constructor.invoke(    {        \"query\": \"What are some sci-fi movies from the 90\\'s directed by Luc Besson about taxi drivers\"    })StructuredQuery(query=\\'taxi driver\\', filter=Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'genre\\', value=\\'science fiction\\'), Operation(operator=<Operator.AND: \\'and\\'>, arguments=[Comparison(comparator=<Comparator.GTE: \\'gte\\'>, attribute=\\'year\\', value=1990), Comparison(comparator=<Comparator.LT: \\'lt\\'>, attribute=\\'year\\', value=2000)]), Comparison(comparator=<Comparator.EQ: \\'eq\\'>, attribute=\\'director\\', value=\\'Luc Besson\\')]), limit=None)The query constructor is the key element of the self-query retriever. To make a great retrieval system you\\'ll need to make sure your query constructor works well. Often this requires adjusting the prompt, the examples in the prompt, the attribute descriptions, etc. For an example that walks through refining a query constructor on some hotel inventory data, check out this cookbook.The next key element is the structured query translator. This is the object responsible for translating the generic StructuredQuery object into a metadata filter in the syntax of the vector store you\\'re using. LangChain comes with a number of built-in translators. To see them all head to the Integrations section.from langchain.retrievers.self_query.chroma import ChromaTranslatorretriever = SelfQueryRetriever(    query_constructor=query_constructor,    vectorstore=vectorstore,    structured_query_translator=ChromaTranslator(),)API Reference:ChromaTranslatorretriever.invoke(    \"What\\'s a movie after 1990 but before 2005 that\\'s all about toys, and preferably is animated\")[Document(page_content=\\'Toys come alive and have a blast doing so\\', metadata={\\'genre\\': \\'animated\\', \\'year\\': 1995})]Help us out by providing feedback on this documentation page:PreviousParent Document RetrieverNextTime-weighted vector store retrieverGet startedCreating our self-querying retrieverTesting it outFilter kConstructing from scratch with LCELCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.',\n",
       " '--- \\n\\n\\n\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | ğŸ¦œï¸�ğŸ”— LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentLangChain v0.2 is out! You are currently viewing the old v0.1 docs. View the latest docs here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1v0.2v0.1ğŸ¦œï¸�ğŸ”—LangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS DocsğŸ’¬SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemğŸ¦œğŸ›\\xa0ï¸� LangSmithğŸ¦œğŸ•¸ï¸� LangGraphğŸ¦œï¸�ğŸ�“ LangServeSecurityExpression LanguageLangChain Expression Language (LCEL)LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together.\\nLCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest â€œprompt + LLMâ€� chain to the most complex chains (weâ€™ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:First-class streaming support\\nWhen you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.Async support\\nAny chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.Optimized parallel execution\\nWhenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.Retries and fallbacks\\nConfigure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. Weâ€™re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.Access intermediate results\\nFor more complex chains itâ€™s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and itâ€™s available on every LangServe server.Input and output schemas\\nInput and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.Seamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.Seamless LangServe deployment\\nAny chain created with LCEL can be easily deployed using LangServe.Help us out by providing feedback on this documentation page:PreviousWeb scrapingNextGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright Â© 2024 LangChain, Inc.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Constrution\n",
    "The clustering approach in tree construction includes a few interesting ideas.\n",
    "\n",
    "### GMM (Gaussian Mixture Model)\n",
    "\n",
    "Model the distribution of data points across different clusters\n",
    "Optimal number of clusters by evaluating the model's Bayesian Information Criterion (BIC)\n",
    "\n",
    "### UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "Supports clustering\n",
    "Reduces the dimensionality of high-dimensional data\n",
    "UMAP helps to highlight the natural grouping of data points based on their similarities\n",
    "\n",
    "### Local and Global Clustering\n",
    "\n",
    "Used to analyze data at different scales\n",
    "Both fine-grained and broader patterns within the data are captured effectively\n",
    "\n",
    "### Thresholding\n",
    "\n",
    "Apply in the context of GMM to determine cluster membership\n",
    "Based on the probability distribution (assignment of data points to ≥ 1 cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform global dimensionality reduction on the embeddings using UMAP.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - n_neighbors: Optional; the number of neighbors to consider for each point.\n",
    "                   If not provided, it defaults to the square root of the number of embeddings.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for the reduced space.\n",
    "    - num_neighbors: The number of neighbors to consider for each point.\n",
    "    - metric: The distance metric to use for UMAP.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the embeddings reduced to the specified dimensionality.\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - max_clusters: The maximum number of clusters to consider.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - An integer representing the optimal number of clusters found.\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster.\n",
    "    - random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple containing the cluster labels and the number of clusters determined.\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n",
    "    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - embeddings: The input embeddings as a numpy array.\n",
    "    - dim: The target dimensionality for UMAP reduction.\n",
    "    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n",
    "\n",
    "    Returns:\n",
    "    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Embeds, clusters, and summarizes a list of texts. This function \n",
    "    first generates embeddings for the texts,\n",
    "    clusters them based on similarity, expands \n",
    "    the cluster assignments for easier processing, \n",
    "    and then summarizes the content within each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: A list of text documents to be processed.\n",
    "    - level: An integer parameter that could define the depth or detail of processing.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple containing two DataFrames:\n",
    "      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n",
    "      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n",
    "         and the cluster identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    ## one doc can be part of multiple clusters.\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Language doc. \n",
    "    \n",
    "    LangChain Expression Language provides a way to compose chain in LangChain.\n",
    "    \n",
    "    Give a detailed summary of the documentation provided.\n",
    "    \n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 1 clusters--\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "leaf_texts = docs_texts\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    "\n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extend all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "\n",
    "# Now, use all_texts to build the vectorstore with Chroma\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=embd)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain Expression Language (LCEL) is a declarative way to compose chains in LangChain, designed to support the transition from prototyping to production without code changes. It offers features like first-class streaming support, parallel execution, retries, fallbacks, and integration with LangSmith and LangServe. A specific code example is not provided in the given context, but LCEL chains can be constructed using components like `PydanticOutputParser` and `SelfQueryRetriever` as demonstrated in the documentation excerpts.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"How to define LCEL? Give me a specific code example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaf_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGatouille makes it as simple to use ColBERT.\n",
    "\n",
    "ColBERT generates a contextually influenced vector for each token in the passages.\n",
    "\n",
    "ColBERT similarly generates vectors for each token in the query.\n",
    "\n",
    "Then, the score of each document is the sum of the maximum similarity of each query embedding to any of the document embeddings:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
