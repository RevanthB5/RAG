{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"...\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ['OPENAI_API_KEY'] = '...'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    bs_kwargs = dict(\n",
    "        parse_only = bs4.SoupStrainer(\n",
    "            class_ = ('post-content','post-title','post-header')\n",
    "        )\n",
    "    ),   \n",
    ")\n",
    "\n",
    "\n",
    "blog_docs = loader.load()\n",
    "\n",
    "## Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "blog_splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=blog_splits,\n",
    "    embedding = OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Query_Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = '''You are an AI language model assistant. \n",
    "Your task is to generate five different \n",
    "versions of the given user question to retrieve \n",
    "relevant documents from a vector database. \n",
    "By generating multiple perspectives on the user question, \n",
    "your goal is to help the user overcome some of the limitations \n",
    "of the distance-based similarity search. Provide these \n",
    "alternative questions separated by newlines. \n",
    "Original question: {question}'''\n",
    "\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    |ChatOpenAI(temperature = 0.9)\n",
    "    |StrOutputParser()\n",
    "    |(lambda x:x.split('\\n')) \n",
    ")\n",
    "# returns list of 5 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "# dumps -> converts each doc to string\n",
    "# loads -> converts each string back to doc\n",
    "def get_unique_union(documents: list[list]):\n",
    "    '''unique union of retrieved documents'''\n",
    "    # flatten list of lists, and convert each Document to json string for comparision\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "\n",
    "\n",
    "    #Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "question = \"what is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)\n",
    "\n",
    "# Retriever brings multiple documents for eact question and stores them in list, \n",
    "# therefore list[list]\n",
    "\n",
    "#The map(list) applies a the retiever to each \n",
    "# element in a list (in this case, the list of \n",
    "# alternative questions) and returns a new collection with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = '''Answer the following questions based on this context:\n",
    "{context}\n",
    "\n",
    "Question : {Ques}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\":retrieval_chain, \"Ques\":itemgetter('question')} ## itemgetter('question) can be replaced by RunnablePassthrough()\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({'question':question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) RAG-Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = '''Your are a helpful assitant that generates multiple search queries based \n",
    "on a single query. \\n\n",
    "Generate multiple Serach queries related to: {question}\\n\n",
    "Output(4 querires):'''\n",
    "\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "generate_queries=(\n",
    "    prompt_rag_fusion\n",
    "    |ChatOpenAI(temperature=0)\n",
    "    |StrOutputParser()\n",
    "    |(lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RRF works by taking the search results from multiple methods, assigning a reciprocal rank score to each document in the results, and then combining the scores to create a new ranking. The concept is that documents appearing in the top positions across multiple search methods are likely to be more relevant and should be ranked higher in the combined result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smaller the rank, greater the relevance. Therefore, inversely proportonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps,loads\n",
    "def reciprocal_rank_fusion(results: list[list],k=60):\n",
    "    '''takes multiple lists of ranked docs\n",
    "    and an optional parameter k used in the RRF formula'''\n",
    "\n",
    "    # Initailize a dict to hold fused scores for each unique documunet\n",
    "    fused_scores = {}\n",
    "\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank(position in the list)\n",
    "        for rank,doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key(assumes documnets can be serialized to JSON) \n",
    "            doc_str = dumps(doc)\n",
    "            # If the dcoument is not yet in the fused_scores dictionary, add it withan initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retreive the current score of the document if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score pf the document using the RRF foumula: 1/(rank+k)\n",
    "            fused_scores[doc_str] += 1/(rank+k)\n",
    "\n",
    "        # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "        reranked_results = [\n",
    "            (loads(doc),score)\n",
    "            for doc,score in sorted(fused_scores.items(), key=lambda x:[1], reverse = True)\n",
    "        ]\n",
    "\n",
    "        return reranked_results\n",
    "    \n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\":question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       "  0.03306010928961749),\n",
       " (Document(page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       "  0.03200204813108039)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down a complicated task into smaller steps that the agent can understand and plan ahead for. This allows the agent to effectively navigate through the task and make informed decisions.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template ='''Answer the followig question based on the context:\n",
    "{context}\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion,\n",
    "     'question': itemgetter('question')}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({'question':question})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Query Translation (Decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 1. answers of one question are relevant to the next question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "#Decomposition\n",
    "template = '''You are a helpful assistant that generates multiple sub-queries related toa n input question.\\n\n",
    "The Goal is to break down the input into a set of sub-problems/ sub-queries that can be answerd in isolation. \\n\n",
    "Generate multiple search queries related to: {question}\\n\n",
    "Output(3 queries):'''\n",
    "\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# LLm\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = (prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split('\\n') ) )\n",
    "\n",
    "# RUN\n",
    "question = 'what are the main components of an LLM-powered autonomous agent system?'\n",
    "\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is LLM technology and how does it work in autonomous agent systems?',\n",
       " '2. What are the key components of an autonomous agent system powered by LLM technology?',\n",
       " '3. How do LLM-powered autonomous agent systems differ from other types of autonomous systems in terms of their components and functionality?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt \n",
    "\n",
    "template = '''Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question}\\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs}\n",
    "\n",
    "here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context}\\n --- \\n\n",
    "\n",
    "Use the above context and any background + answer pairs to answer the question: \\n {question}\n",
    "'''\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question,answer):\n",
    "    '''Format Q and A pair'''\n",
    "\n",
    "    formatted_string = ''\n",
    "    formatted_string += f\"Question: {question}\\n Answer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0)\n",
    "\n",
    "#\n",
    "q_a_pairs = ''\n",
    "for q in questions:\n",
    "    rag_chain = (\n",
    "        {'context':itemgetter('question')|retriever,\n",
    "         'question': itemgetter('question'),\n",
    "         'q_a_pairs': itemgetter('q_a_pairs')}\n",
    "         |decomposition_prompt\n",
    "         |llm\n",
    "         |StrOutputParser()\n",
    "    )\n",
    "\n",
    "    answer = rag_chain.invoke({'question':q,'q_a_pairs':q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + '\\n------\\n'+ q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-powered autonomous agent systems differ from other types of autonomous systems in terms of their components and functionality by having LLM technology as a central component. LLM functions as the agent's brain, enabling it to process and understand human language inputs, make informed decisions, and carry out tasks effectively. This advanced AI model allows the agent to interpret and respond to natural language inputs, which is a unique feature not commonly found in other autonomous systems. Additionally, LLM-powered autonomous agent systems often include planning mechanisms to help the agent decompose complex tasks, plan ahead, and execute tasks in a structured manner. These components work together to enhance the agent's cognitive abilities, decision-making processes, and task execution capabilities, setting them apart from other autonomous systems that may not have such advanced language processing and planning capabilities.\n"
     ]
    }
   ],
   "source": [
    "print(answer) #last answer in the list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 2. Sub-answers are not related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. What is LLM technology and how does it work in autonomous agent systems?', '2. What are the key components of an autonomous agent system powered by LLM technology?', '3. How do the main components of an LLM-powered autonomous agent system interact with each other to enable autonomous behavior?']\n",
      "1. What is LLM technology and how does it work in autonomous agent systems?\n",
      "2. What are the key components of an autonomous agent system powered by LLM technology?\n",
      "3. How do the main components of an LLM-powered autonomous agent system interact with each other to enable autonomous behavior?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    '''RAG on each sub-question'''\n",
    "\n",
    "    # Use decompositiom\n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "\n",
    "    # Initialize a list to hold RAG chain results\n",
    "\n",
    "    rag_results = []\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "\n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "\n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({'context':retrieved_docs,\n",
    "                                                                'question':sub_question})\n",
    "        rag_results.append(answer)\n",
    "\n",
    "    return rag_results, sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers , questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What is LLM technology and how does it work in autonomous agent systems? \n",
      " LLM technology functions as the brain of autonomous agent systems, supported by key components. In these systems, LLM helps the agent plan and execute tasks by decomposing complex tasks into manageable steps. It serves as a central component in guiding the agent's decision-making and actions.\n",
      "Question: 1. What is LLM technology and how does it work in autonomous agent systems?\n",
      " Answer: LLM technology functions as the brain of autonomous agent systems, supported by key components. In these systems, LLM helps the agent plan and execute tasks by decomposing complex tasks into manageable steps. It serves as a central component in guiding the agent's decision-making and actions.\n",
      "\n",
      "\n",
      "2. What are the key components of an autonomous agent system powered by LLM technology? \n",
      " The key components of an autonomous agent system powered by LLM technology include LLM as the agent's brain and other components such as planning and task decomposition. Planning is essential for the agent to understand and prepare for the steps involved in a complex task. Task decomposition helps break down tasks into manageable parts for the agent to execute effectively.\n",
      "Question: 1. What is LLM technology and how does it work in autonomous agent systems?\n",
      " Answer: LLM technology functions as the brain of autonomous agent systems, supported by key components. In these systems, LLM helps the agent plan and execute tasks by decomposing complex tasks into manageable steps. It serves as a central component in guiding the agent's decision-making and actions.\n",
      "\n",
      "Question: 2. What are the key components of an autonomous agent system powered by LLM technology?\n",
      " Answer: The key components of an autonomous agent system powered by LLM technology include LLM as the agent's brain and other components such as planning and task decomposition. Planning is essential for the agent to understand and prepare for the steps involved in a complex task. Task decomposition helps break down tasks into manageable parts for the agent to execute effectively.\n",
      "\n",
      "\n",
      "3. How do the main components of an LLM-powered autonomous agent system interact with each other to enable autonomous behavior? \n",
      " In a LLM-powered autonomous agent system, the LLM functions as the agent's brain and is complemented by key components like planning. Planning involves task decomposition to break down complicated tasks into manageable steps. These components interact to enable autonomous behavior by guiding the agent's decision-making and actions.\n",
      "Question: 1. What is LLM technology and how does it work in autonomous agent systems?\n",
      " Answer: LLM technology functions as the brain of autonomous agent systems, supported by key components. In these systems, LLM helps the agent plan and execute tasks by decomposing complex tasks into manageable steps. It serves as a central component in guiding the agent's decision-making and actions.\n",
      "\n",
      "Question: 2. What are the key components of an autonomous agent system powered by LLM technology?\n",
      " Answer: The key components of an autonomous agent system powered by LLM technology include LLM as the agent's brain and other components such as planning and task decomposition. Planning is essential for the agent to understand and prepare for the steps involved in a complex task. Task decomposition helps break down tasks into manageable parts for the agent to execute effectively.\n",
      "\n",
      "Question: 3. How do the main components of an LLM-powered autonomous agent system interact with each other to enable autonomous behavior?\n",
      " Answer: In a LLM-powered autonomous agent system, the LLM functions as the agent's brain and is complemented by key components like planning. Planning involves task decomposition to break down complicated tasks into manageable steps. These components interact to enable autonomous behavior by guiding the agent's decision-making and actions.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The main components of an LLM-powered autonomous agent system include the LLM technology serving as the agent's brain, planning for task understanding and preparation, and task decomposition for breaking down complex tasks into manageable steps. These components work together to enable autonomous behavior by guiding the agent's decision-making and actions in executing tasks effectively.\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions,answers):\n",
    "\n",
    "    formatted_string = ''\n",
    "    for (question,answer) in zip(questions,answers):\n",
    "        formatted_string += f\"Question: {question}\\n Answer: {answer}\\n\\n\"\n",
    "        print(question,'\\n',answer)\n",
    "        print(formatted_string)\n",
    "    return formatted_string.strip()\n",
    "    \n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "\n",
    "template = '''Here is the set of Q+A pairs:\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "        prompt|\n",
    "        llm|\n",
    "        StrOutputParser()\n",
    "    )\n",
    "    \n",
    "final_rag_chain.invoke({\"context\":context ,'question':question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Query Translation (step-back)\n",
    "output more generic questions which are easier to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents refers to the process of breaking down large and complex tasks into smaller, more manageable subgoals. This approach enables LLM-powered autonomous agents to efficiently handle intricate tasks by dividing them into smaller components that can be tackled sequentially or in parallel. By decomposing tasks, LLM agents can better understand the steps involved in completing a task and plan ahead accordingly. This process helps the agents to navigate through various dependencies and requirements associated with the task, leading to more effective and successful task execution.\\n\\nTask decomposition is a crucial component of the overall functioning of LLM-powered autonomous agent systems. It allows the agents to parse user requests, identify the different tasks required to fulfill those requests, and organize them in a structured manner. By utilizing few-shot examples and leveraging the capabilities of LLM as the brain of the system, task decomposition ensures that the agents can effectively plan and execute tasks in a systematic and efficient manner.\\n\\nIn summary, task decomposition for LLM agents plays a vital role in enabling these autonomous agents to handle complex tasks by breaking them down into smaller subgoals, facilitating better task planning, organization, and execution.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "generate_queries_step_back = prompt | ChatOpenAI(temperature=0) | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})\n",
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. \n",
    "I am going to ask you a question. Your response should be comprehensive \n",
    "and not contradicted with the following context if they \n",
    "are relevant. Otherwise, ignore them if \n",
    "they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Query Translation (HyDE)\n",
    "\n",
    "-- the users questions are converted to a hypothetical document that is mapped to the real document.\n",
    "\n",
    "-- Because the hypothetical document will be closer to the real document than the user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a fundamental concept in the field of machine learning and artificial intelligence, particularly for agents utilizing the Long-Short Term Memory (LSTM) architecture. Task decomposition refers to the process of breaking down a complex task into smaller, more manageable sub-tasks that can be individually learned and executed by the agent. \\n\\nIn the context of LLM agents, task decomposition is crucial for improving the efficiency and effectiveness of the learning process. By breaking down a complex task into smaller sub-tasks, the agent can focus on learning and optimizing each sub-task separately, before combining them to perform the overall task. This approach allows the agent to learn more quickly and accurately, as it can leverage its memory capabilities to retain information about each sub-task and apply it to the larger task.\\n\\nFurthermore, task decomposition can also help LLM agents generalize better to new, unseen tasks. By learning a set of basic sub-tasks that are common across different tasks, the agent can transfer its knowledge and skills to new tasks more easily. This can lead to improved performance and adaptability in a variety of real-world scenarios.\\n\\nOverall, task decomposition plays a critical role in the development and optimization of LLM agents, enabling them to effectively learn and execute complex tasks by breaking them down into smaller, more manageable components.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE \n",
    "template = '''please write a scientifc paper passge to answer the question\n",
    "Question:{question}\n",
    "Passage:'''\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = 'what is task decomposition for LLM agents?'\n",
    "generate_docs_for_retrieval.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = generate_docs_for_retrieval | retriever\n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This process allows the agent to plan ahead and understand the steps involved in completing a complicated task.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "\n",
    "template = '''Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt|\n",
    "    llm|\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retrieved_docs, 'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
